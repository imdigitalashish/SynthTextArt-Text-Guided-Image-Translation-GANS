{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aOrnnxgD698X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel, stride=2):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channel, out_channel, 4, stride, bias=False, padding_mode=\"reflect\"),\n",
        "        nn.BatchNorm2d(out_channel),\n",
        "        nn.LeakyReLU(0.2)\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "# x, y <- concatenate these along the channels\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3, features=[64, 128, 256, 512]): # 256 input -> 30x30 output\n",
        "    super().__init__()\n",
        "    self.initial = nn.Sequential(\n",
        "        nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "    layers = []\n",
        "    in_channels = features[0]\n",
        "    for feature in features[1:]:\n",
        "      layers.append(\n",
        "          CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2)\n",
        "      )\n",
        "      in_channels = feature\n",
        "\n",
        "    layers.append(\n",
        "        nn.Conv2d(\n",
        "            in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    x = torch.cat([x,y], dim=1)\n",
        "    x = self.initial(x)\n",
        "    return self.model(x)\n",
        "\n",
        "\n",
        "def test():\n",
        "  x = torch.randn((1, 3, 256, 256))\n",
        "  y = torch.randn((1, 3, 256, 256))\n",
        "  model = Discriminator()\n",
        "  preds = model(x,y)\n",
        "  print(preds.shape)\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X-zNdzN7MfU",
        "outputId": "f676e7e1-f274-42fa-9717-b10718e2043a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 26, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNET\n"
      ],
      "metadata": {
        "id": "R18NaZWk91fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 4,2,1,bias=False,padding_mode=\"reflect\"),\n",
        "        if down\n",
        "        else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2)\n",
        "    )"
      ],
      "metadata": {
        "id": "c5OJVBql9K-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}