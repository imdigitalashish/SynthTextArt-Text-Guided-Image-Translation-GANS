{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":212894,"sourceType":"datasetVersion","datasetId":91717}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"id":"aOrnnxgD698X","execution":{"iopub.status.busy":"2023-11-23T19:03:51.956107Z","iopub.execute_input":"2023-11-23T19:03:51.956486Z","iopub.status.idle":"2023-11-23T19:03:51.961078Z","shell.execute_reply.started":"2023-11-23T19:03:51.956452Z","shell.execute_reply":"2023-11-23T19:03:51.960135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTRAIN_DIR = \"data/train\"\nVAL_DIR = \"data/val\"\nLEARNING_RATE = 2e-4\nBATCH_SIZE = 16\nNUM_WORKERS = 2\nIMAGE_SIZE = 256\nCHANNELS_IMG = 3\nL1_LAMBDA = 100\nLAMBDA_GP = 10\nNUM_EPOCHS = 500\nLOAD_MODEL = True\nSAVE_MODEL = True\nCHECKPOINT_DISC = \"disc.pth.tar\"\nCHECKPOINT_GEN = \"gen.pth.tar\"\n\nboth_transform = A.Compose(\n    [A.Resize(width=256, height=256),], additional_targets={\"image0\": \"image\"},is_check_shapes=False\n)\n\ntransform_only_input = A.Compose(\n    [\n        A.HorizontalFlip(p=0.5),\n        A.ColorJitter(p=0.2),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n        ToTensorV2(),\n    ],\n    is_check_shapes=False\n)\n\ntransform_only_mask = A.Compose(\n    [\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n        ToTensorV2(),\n    ],\n    is_check_shapes=False\n)","metadata":{"id":"jc2yTRQSnimF","execution":{"iopub.status.busy":"2023-11-23T19:03:52.427234Z","iopub.execute_input":"2023-11-23T19:03:52.428261Z","iopub.status.idle":"2023-11-23T19:03:54.197997Z","shell.execute_reply.started":"2023-11-23T19:03:52.428213Z","shell.execute_reply":"2023-11-23T19:03:54.196929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UTILS\n\nimport torch\nfrom torchvision.utils import save_image\n\ndef save_some_examples(gen, val_loader, epoch, folder):\n    x, y = next(iter(val_loader))\n    x, y = x.to(DEVICE), y.to(DEVICE)\n    gen.eval()\n    with torch.no_grad():\n        y_fake = gen(x)\n        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n        if epoch == 1:\n            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n    gen.train()\n\n\ndef save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n\n","metadata":{"id":"L11vK7fZresw","execution":{"iopub.status.busy":"2023-11-23T19:03:54.200155Z","iopub.execute_input":"2023-11-23T19:03:54.200643Z","iopub.status.idle":"2023-11-23T19:03:54.210576Z","shell.execute_reply.started":"2023-11-23T19:03:54.200591Z","shell.execute_reply":"2023-11-23T19:03:54.209652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n  def __init__(self, in_channel, out_channel, stride=2):\n    super().__init__()\n    self.conv = nn.Sequential(\n        nn.Conv2d(in_channel, out_channel, 4, stride, bias=False, padding_mode=\"reflect\"),\n        nn.BatchNorm2d(out_channel),\n        nn.LeakyReLU(0.2)\n\n    )\n\n  def forward(self, x):\n    return self.conv(x)\n\n\n# x, y <- concatenate these along the channels\n\nclass Discriminator(nn.Module):\n  def __init__(self, in_channels=3, features=[64, 128, 256, 512]): # 256 input -> 30x30 output\n    super().__init__()\n    self.initial = nn.Sequential(\n        nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n        nn.LeakyReLU(0.2)\n    )\n    layers = []\n    in_channels = features[0]\n    for feature in features[1:]:\n      layers.append(\n          CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2)\n      )\n      in_channels = feature\n\n    layers.append(\n        nn.Conv2d(\n            in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n        )\n    )\n\n    self.model = nn.Sequential(*layers)\n\n  def forward(self, x, y):\n    x = torch.cat([x,y], dim=1)\n    x = self.initial(x)\n    return self.model(x)\n\n\ndef test():\n  x = torch.randn((1, 3, 256, 256))\n  y = torch.randn((1, 3, 256, 256))\n  model = Discriminator()\n  preds = model(x,y)\n  print(preds.shape)\n\ntest()","metadata":{"id":"3X-zNdzN7MfU","outputId":"f874a29e-ea85-48b4-d798-b32c3f0257af","execution":{"iopub.status.busy":"2023-11-23T19:03:54.212069Z","iopub.execute_input":"2023-11-23T19:03:54.212362Z","iopub.status.idle":"2023-11-23T19:03:54.453111Z","shell.execute_reply.started":"2023-11-23T19:03:54.212339Z","shell.execute_reply":"2023-11-23T19:03:54.452160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNET\n","metadata":{"id":"R18NaZWk91fa"}},{"cell_type":"code","source":"class Block(nn.Module):\n  def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n    super().__init__()\n    self.conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 4,2,1,bias=False,padding_mode=\"reflect\")\n        if down\n        else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(out_channels),\n        # nn.InstanceNorm2d(out_channels, affine=True), # ACCORDING TO CYCLE GAN PAPER\n        nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n    )\n    self.use_dropout = use_dropout\n    self.dropout = nn.Dropout(0.5)\n\n  def forward(self, x):\n    x = self.conv(x)\n    return self.dropout(x) if self.use_dropout else x\n\n\nclass Generator(nn.Module):\n  def __init__(self, in_channels=3, features=64):\n    super().__init__()\n    self.initial_down = nn.Sequential(\n        nn.Conv2d(in_channels, features, 4,2,1, padding_mode=\"reflect\"),\n        nn.LeakyReLU(0.2)\n    )\n    # features * 1 = 64, features * 2 = 128 model arch\n    self.down1 = Block(features, features*2, down=True, act=\"leaky\", use_dropout=False) #64\n    self.down2 = Block(features*2, features*4, down=True, act=\"leaky\", use_dropout=False) #32\n    self.down3 = Block(features*4, features*8, down=True, act=\"leaky\", use_dropout=False) #16\n    self.down4 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False) #8\n    self.down5 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False) #4\n    self.down6 = Block(features*8, features*8, down=True, act=\"leaky\", use_dropout=False) #2\n    self.bottleneck = nn.Sequential(\n        nn.Conv2d(features*8, features*8, 4, 2, 1, padding_mode=\"reflect\"), nn.ReLU(),\n    )\n    self.up1 = Block(features*8, features*8, down=False, act=\"relu\", use_dropout=True)\n    self.up2 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)\n    self.up3 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)\n    self.up4 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=False)\n    self.up5 = Block(features*8*2, features*4, down=False, act=\"relu\", use_dropout=False)\n    self.up6 = Block(features*4*2, features*2, down=False, act=\"relu\", use_dropout=False)\n    self.up7 = Block(features*2*2, features, down=False, act=\"relu\", use_dropout=False)\n    self.final_up = nn.Sequential(\n        nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1),\n        nn.Tanh(),\n    )\n\n  def forward(self,x):\n    d1 = self.initial_down(x)\n    d2 = self.down1(d1)\n    d3 = self.down2(d2)\n    d4 = self.down3(d3)\n    d5 = self.down4(d4)\n    d6 = self.down5(d5)\n    d7 = self.down6(d6)\n    bottleneck = self.bottleneck(d7)\n    up1 = self.up1(bottleneck)\n    up2 = self.up2(torch.cat([up1, d7], 1))\n    up3 = self.up3(torch.cat([up2, d6], 1))\n    up4 = self.up4(torch.cat([up3, d5], 1))\n    up5 = self.up5(torch.cat([up4, d4], 1))\n    up6 = self.up6(torch.cat([up5, d3], 1))\n    up7 = self.up7(torch.cat([up6, d2], 1))\n\n    return self.final_up(torch.cat([up7, d1], 1))\n\n\n\ndef test():\n  x = torch.randn((1,3,256,256))\n  model = Generator(in_channels=3, features=64)\n  preds = model(x)\n  print(preds.shape)\n\ntest()\n\n\n\n\n","metadata":{"id":"c5OJVBql9K-t","outputId":"efff7c78-c326-47b6-afaf-7fa531bcf5fe","execution":{"iopub.status.busy":"2023-11-23T19:03:54.455976Z","iopub.execute_input":"2023-11-23T19:03:54.456350Z","iopub.status.idle":"2023-11-23T19:03:55.188382Z","shell.execute_reply.started":"2023-11-23T19:03:54.456316Z","shell.execute_reply":"2023-11-23T19:03:55.187440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Loading","metadata":{"id":"WQN_rCwclh9z"}},{"cell_type":"code","source":"# !pip install -q kaggle","metadata":{"id":"iS74dShKl8hG","execution":{"iopub.status.busy":"2023-11-23T19:03:55.189771Z","iopub.execute_input":"2023-11-23T19:03:55.190077Z","iopub.status.idle":"2023-11-23T19:03:55.194709Z","shell.execute_reply.started":"2023-11-23T19:03:55.190051Z","shell.execute_reply":"2023-11-23T19:03:55.193668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls -lha kaggle.json\n# !mkdir -p ~/.kaggle\n# !cp kaggle.json ~/.kaggle/\n\n# !kaggle datasets download -d ktaebum/anime-sketch-colorization-pair","metadata":{"id":"ow9b8n8Yl-i0","outputId":"fd1212cc-d22b-46f9-d32f-4cf9851f4872","execution":{"iopub.status.busy":"2023-11-23T19:03:55.196051Z","iopub.execute_input":"2023-11-23T19:03:55.196402Z","iopub.status.idle":"2023-11-23T19:03:55.204350Z","shell.execute_reply.started":"2023-11-23T19:03:55.196375Z","shell.execute_reply":"2023-11-23T19:03:55.203333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip anime-sketch-colorization-pair.zip","metadata":{"id":"Mx3Qe7AqmVeT","outputId":"38b871a0-e262-46f9-b2b1-64e1e4eb819a","execution":{"iopub.status.busy":"2023-11-23T19:03:55.205444Z","iopub.execute_input":"2023-11-23T19:03:55.205805Z","iopub.status.idle":"2023-11-23T19:03:55.216023Z","shell.execute_reply.started":"2023-11-23T19:03:55.205771Z","shell.execute_reply":"2023-11-23T19:03:55.215150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE","metadata":{"id":"FIgMAnMfpiqX","outputId":"c7e5c228-7810-44d7-b8fe-f02fda8bc8be","execution":{"iopub.status.busy":"2023-11-23T19:03:55.217930Z","iopub.execute_input":"2023-11-23T19:03:55.218209Z","iopub.status.idle":"2023-11-23T19:03:55.227815Z","shell.execute_reply.started":"2023-11-23T19:03:55.218185Z","shell.execute_reply":"2023-11-23T19:03:55.226929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport os\nfrom torch.utils.data import Dataset\n\nclass MapDataset(Dataset):\n    def __init__(self, root_dir):\n        self.root_dir = root_dir\n        self.list_files = os.listdir(self.root_dir)\n\n    def __len__(self):\n        return len(self.list_files)\n\n    def __getitem__(self, index):\n        img_file = self.list_files[index]\n        img_path = os.path.join(self.root_dir, img_file)\n        image = np.array(Image.open(img_path))\n        input_image = image[:, 600:, :]\n        target_image = image[:, :600, :]\n\n        augmentations = both_transform(image=input_image, image0=target_image)\n        input_image = augmentations[\"image\"]\n        target_image = augmentations[\"image0\"]\n\n        input_image = transform_only_input(image=input_image)[\"image\"]\n        target_image = transform_only_mask(image=target_image)[\"image\"]\n\n        return input_image, target_image\n\ndataset = MapDataset(\"/kaggle/input/anime-sketch-colorization-pair/data/train\")\nlen(dataset)\n# loader = DataLoader(dataset, batch_size=5)\n# for x, y in loader:\n#     print(x.shape)\n#     save_image(x, \"x.png\")\n#     save_image(y, \"y.png\")\n#     import sys\n\n#     sys.exit()","metadata":{"id":"bxn8jwBslAF0","outputId":"c111e451-9c8c-418a-b4af-88d8b12f6b16","execution":{"iopub.status.busy":"2023-11-23T19:03:55.229113Z","iopub.execute_input":"2023-11-23T19:03:55.229559Z","iopub.status.idle":"2023-11-23T19:03:55.451501Z","shell.execute_reply.started":"2023-11-23T19:03:55.229525Z","shell.execute_reply":"2023-11-23T19:03:55.450575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAIN","metadata":{"id":"ZKV6wUHqrwqx"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm # PROGRESS BAR\nfrom torchvision.utils import save_image\n\ntorch.backends.cudnn.benchmark = True\n\ndef train_fn(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler):\n  loop = tqdm(loader, leave=True)\n\n  for idx, (x, y) in enumerate(loop):\n      x = x.to(DEVICE)\n      y = y.to(DEVICE)\n\n      # Train Discriminator\n      with torch.cuda.amp.autocast():\n          y_fake = gen(x)\n          D_real = disc(x, y)\n          D_real_loss = bce(D_real, torch.ones_like(D_real))\n          D_fake = disc(x, y_fake.detach())\n          D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n          D_loss = (D_real_loss + D_fake_loss) / 2\n\n      disc.zero_grad()\n      d_scaler.scale(D_loss).backward()\n      d_scaler.step(opt_disc)\n      d_scaler.update()\n\n      # Train generator\n      with torch.cuda.amp.autocast():\n          D_fake = disc(x, y_fake)\n          G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n          L1 = l1_loss(y_fake, y) * L1_LAMBDA # SPECIFIED IN PAPER TO COMPUTE L1 LOSS\n          G_loss = G_fake_loss + L1\n\n      opt_gen.zero_grad()\n      g_scaler.scale(G_loss).backward()\n      g_scaler.step(opt_gen)\n      g_scaler.update()\n\n      if idx % 10 == 0:\n          loop.set_postfix(\n              D_real=torch.sigmoid(D_real).mean().item(),\n              D_fake=torch.sigmoid(D_fake).mean().item(),\n          )\n    #loss.backward(retain_graph=True) alternative to y_fake.detach() for avoid breaking the computational graph\n\n\ndef main():\n  disc = Discriminator(in_channels=3).to(DEVICE)\n  gen = Generator(in_channels=3).to(DEVICE)\n  opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n  opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n  BCE = nn.BCEWithLogitsLoss()\n  L1_LOSS = nn.L1Loss() # You just do different combinations to check which is best and authors already specified that\n\n  if LOAD_MODEL:\n    load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n    load_checkpoint(CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE)\n\n  train_dataset = MapDataset(root_dir=\"/kaggle/input/anime-sketch-colorization-pair/data/train\")\n  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n\n  # FOR LOW VRAM\n  g_scaler = torch.cuda.amp.GradScaler()\n  d_scaler = torch.cuda.amp.GradScaler()\n\n  val_dataset = MapDataset(root_dir=\"/kaggle/input/anime-sketch-colorization-pair/data/val\") # Validation dataset\n  val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n  for epoch in range(50+1, NUM_EPOCHS):\n    print(f\"EPOCH: {str(epoch)} => \")\n    train_fn(disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler)\n\n    if SAVE_MODEL and epoch % 5 == 0:\n      save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n      save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n\n    save_some_examples(gen, val_loader, epoch, folder=\"evaluation\") # if ERROR: Create a folder evaluation\n\n\n\nmain()\n","metadata":{"id":"sVhCHJdHrFJf","outputId":"1794bbd0-edff-4bee-ded3-cc701996cdba","execution":{"iopub.status.busy":"2023-11-23T19:03:55.543273Z","iopub.execute_input":"2023-11-23T19:03:55.543576Z","iopub.status.idle":"2023-11-23T20:54:09.339741Z","shell.execute_reply.started":"2023-11-23T19:03:55.543550Z","shell.execute_reply":"2023-11-23T20:54:09.338088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls\n!zip evaluation.zip -r evaluation","metadata":{"execution":{"iopub.status.busy":"2023-11-23T20:54:13.318441Z","iopub.execute_input":"2023-11-23T20:54:13.319285Z","iopub.status.idle":"2023-11-23T20:54:15.586178Z","shell.execute_reply.started":"2023-11-23T20:54:13.319243Z","shell.execute_reply":"2023-11-23T20:54:15.585226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nnum_workers = os.cpu_count()\nnum_workers","metadata":{"id":"HT-XOtdNuWJv","outputId":"6d686182-1c60-48e5-d21a-16a84db13426","execution":{"iopub.status.busy":"2023-11-22T14:31:08.938342Z","iopub.execute_input":"2023-11-22T14:31:08.938750Z","iopub.status.idle":"2023-11-22T14:31:08.945247Z","shell.execute_reply.started":"2023-11-22T14:31:08.938722Z","shell.execute_reply":"2023-11-22T14:31:08.944140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo rm -r evaluation/*.png","metadata":{"id":"Hc8gAQxxuXPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/anime-sketch-colorization-pair/data/train","metadata":{"id":"7IAU9JU56q_f","execution":{"iopub.status.busy":"2023-11-22T14:30:12.892402Z","iopub.execute_input":"2023-11-22T14:30:12.892815Z","iopub.status.idle":"2023-11-22T14:30:14.374438Z","shell.execute_reply.started":"2023-11-22T14:30:12.892781Z","shell.execute_reply":"2023-11-22T14:30:14.373354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
